{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and Preprocessing Data\n",
      "Keys in file: ['X', 'Y']\n",
      "=== DATA: ===\n",
      "[[  9   0   0 ...   2 253  18]\n",
      " [  9   0   4 ...   5 268  13]\n",
      " [ 10   0   0 ...   0 286  45]\n",
      " ...\n",
      " [ 20   0   2 ...   1 200  21]\n",
      " [ 19   4   0 ...   2 211  19]\n",
      " [  0   0   0 ...   2 170  21]]\n",
      "\n",
      "\n",
      "=== DATA AFTER: ===\n",
      "[[ 1.37633088 -0.07424526 -0.53139794 ... -0.82777124  0.13916345\n",
      "  -0.08761217]\n",
      " [ 0.15100039  0.32316539 -0.53139794 ... -1.31165484 -0.6785124\n",
      "   0.86436923]\n",
      " [-1.05594923 -0.77410053 -0.53139794 ... -2.02494913 -0.82194957\n",
      "  -0.09430186]\n",
      " ...\n",
      " [ 0.70618508 -0.11371273 -0.53139794 ...  0.49143894 -1.80315314\n",
      "   0.9931173 ]\n",
      " [-0.91948786  0.07881428 -0.53139794 ...  1.539124   -0.64641285\n",
      "  -1.82330624]\n",
      " [ 0.59652344 -0.40862739  0.94749624 ...  0.87469364 -0.12875263\n",
      "   0.19072659]]\n",
      "\n",
      "\n",
      "Defining and Training Model\n",
      "Pretraining autoencoder...\n",
      "Pretrain Epoch: 10, Loss: 0.8773766756057739\n",
      "Pretrain Epoch: 20, Loss: 0.9208022952079773\n",
      "Pretrain Epoch: 30, Loss: 0.8740976452827454\n",
      "Pretrain Epoch: 40, Loss: 0.8543938398361206\n",
      "Pretrain Epoch: 50, Loss: 0.8228693008422852\n",
      "Pretrain Epoch: 60, Loss: 0.839730441570282\n",
      "Pretrain Epoch: 70, Loss: 0.8193323016166687\n",
      "Pretrain Epoch: 80, Loss: 0.8135716319084167\n",
      "Pretrain Epoch: 90, Loss: 0.8410630822181702\n",
      "Pretrain Epoch: 100, Loss: 0.8510115742683411\n",
      "Epoch 1/100, Average Loss: 0.0269\n",
      "Epoch 2/100, Average Loss: 0.0278\n",
      "Epoch 3/100, Average Loss: 0.0285\n",
      "Epoch 4/100, Average Loss: 0.0292\n",
      "Epoch 5/100, Average Loss: 0.0298\n",
      "Epoch 6/100, Average Loss: 0.0303\n",
      "Epoch 7/100, Average Loss: 0.0306\n",
      "Epoch 8/100, Average Loss: 0.0308\n",
      "Epoch 9/100, Average Loss: 0.0307\n",
      "Epoch 10/100, Average Loss: 0.0304\n",
      "Epoch 11/100, Average Loss: 0.0302\n",
      "Epoch 12/100, Average Loss: 0.0301\n",
      "Epoch 13/100, Average Loss: 0.0299\n",
      "Epoch 14/100, Average Loss: 0.0298\n",
      "Epoch 15/100, Average Loss: 0.0297\n",
      "Epoch 16/100, Average Loss: 0.0296\n",
      "Epoch 17/100, Average Loss: 0.0295\n",
      "Epoch 18/100, Average Loss: 0.0294\n",
      "Epoch 19/100, Average Loss: 0.0293\n",
      "Epoch 20/100, Average Loss: 0.0292\n",
      "Epoch 21/100, Average Loss: 0.0291\n",
      "Epoch 22/100, Average Loss: 0.0291\n",
      "Epoch 23/100, Average Loss: 0.0289\n",
      "Epoch 24/100, Average Loss: 0.0289\n",
      "Epoch 25/100, Average Loss: 0.0288\n",
      "Epoch 26/100, Average Loss: 0.0287\n",
      "Epoch 27/100, Average Loss: 0.0286\n",
      "Epoch 28/100, Average Loss: 0.0286\n",
      "Epoch 29/100, Average Loss: 0.0285\n",
      "Epoch 30/100, Average Loss: 0.0285\n",
      "Epoch 31/100, Average Loss: 0.0284\n",
      "Epoch 32/100, Average Loss: 0.0283\n",
      "Epoch 33/100, Average Loss: 0.0283\n",
      "Epoch 34/100, Average Loss: 0.0282\n",
      "Epoch 35/100, Average Loss: 0.0282\n",
      "Epoch 36/100, Average Loss: 0.0281\n",
      "Epoch 37/100, Average Loss: 0.0281\n",
      "Epoch 38/100, Average Loss: 0.0280\n",
      "Epoch 39/100, Average Loss: 0.0280\n",
      "Epoch 40/100, Average Loss: 0.0279\n",
      "Epoch 41/100, Average Loss: 0.0279\n",
      "Epoch 42/100, Average Loss: 0.0278\n",
      "Epoch 43/100, Average Loss: 0.0278\n",
      "Epoch 44/100, Average Loss: 0.0277\n",
      "Epoch 45/100, Average Loss: 0.0277\n",
      "Epoch 46/100, Average Loss: 0.0277\n",
      "Epoch 47/100, Average Loss: 0.0276\n",
      "Epoch 48/100, Average Loss: 0.0276\n",
      "Epoch 49/100, Average Loss: 0.0276\n",
      "Epoch 50/100, Average Loss: 0.0275\n",
      "Epoch 51/100, Average Loss: 0.0275\n",
      "Epoch 52/100, Average Loss: 0.0274\n",
      "Epoch 53/100, Average Loss: 0.0274\n",
      "Epoch 54/100, Average Loss: 0.0274\n",
      "Epoch 55/100, Average Loss: 0.0274\n",
      "Epoch 56/100, Average Loss: 0.0274\n",
      "Epoch 57/100, Average Loss: 0.0273\n",
      "Epoch 58/100, Average Loss: 0.0273\n",
      "Epoch 59/100, Average Loss: 0.0273\n",
      "Epoch 60/100, Average Loss: 0.0273\n",
      "Epoch 61/100, Average Loss: 0.0273\n",
      "Epoch 62/100, Average Loss: 0.0272\n",
      "Epoch 63/100, Average Loss: 0.0272\n",
      "Epoch 64/100, Average Loss: 0.0272\n",
      "Epoch 65/100, Average Loss: 0.0272\n",
      "Epoch 66/100, Average Loss: 0.0271\n",
      "Epoch 67/100, Average Loss: 0.0271\n",
      "Epoch 68/100, Average Loss: 0.0271\n",
      "Epoch 69/100, Average Loss: 0.0270\n",
      "Epoch 70/100, Average Loss: 0.0270\n",
      "Epoch 71/100, Average Loss: 0.0270\n",
      "Epoch 72/100, Average Loss: 0.0270\n",
      "Epoch 73/100, Average Loss: 0.0270\n",
      "Epoch 74/100, Average Loss: 0.0270\n",
      "Epoch 75/100, Average Loss: 0.0270\n",
      "Epoch 76/100, Average Loss: 0.0270\n",
      "Epoch 77/100, Average Loss: 0.0270\n",
      "Epoch 78/100, Average Loss: 0.0269\n",
      "Epoch 79/100, Average Loss: 0.0269\n",
      "Epoch 80/100, Average Loss: 0.0269\n",
      "Epoch 81/100, Average Loss: 0.0269\n",
      "Epoch 82/100, Average Loss: 0.0269\n",
      "Epoch 83/100, Average Loss: 0.0268\n",
      "Epoch 84/100, Average Loss: 0.0268\n",
      "Epoch 85/100, Average Loss: 0.0268\n",
      "Epoch 86/100, Average Loss: 0.0268\n",
      "Epoch 87/100, Average Loss: 0.0268\n",
      "Epoch 88/100, Average Loss: 0.0268\n",
      "Epoch 89/100, Average Loss: 0.0268\n",
      "Epoch 90/100, Average Loss: 0.0268\n",
      "Epoch 91/100, Average Loss: 0.0267\n",
      "Epoch 92/100, Average Loss: 0.0267\n",
      "Epoch 93/100, Average Loss: 0.0267\n",
      "Epoch 94/100, Average Loss: 0.0267\n",
      "Epoch 95/100, Average Loss: 0.0267\n",
      "Epoch 96/100, Average Loss: 0.0267\n",
      "Epoch 97/100, Average Loss: 0.0267\n",
      "Epoch 98/100, Average Loss: 0.0267\n",
      "Epoch 99/100, Average Loss: 0.0267\n",
      "Epoch 100/100, Average Loss: 0.0267\n",
      "Obtaining Embeddings\n",
      "Clustering\n",
      "Clustering Evaluation: ARI=0.0471, NMI=0.0441\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from data_loader import load_data, preprocess\n",
    "from model import Autoencoder, ClusteringLayer \n",
    "from trainer import train, get_embeddings\n",
    "from cluster import evaluate_cluster\n",
    "import torch\n",
    "\n",
    "def main(filepath, file_format, n_clusters, encoding_dim, epochs, learning_rate, batch_size, normalize, scale, log_transform, n_top_genes,pretrain_epochs=200,ae_weights=None):\n",
    "    #load and preprocess Data\n",
    "    print(\"Loading and Preprocessing Data\")\n",
    "    data, labels = load_data(filepath, format=file_format)\n",
    "    if data is None:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    processed_data, scaler = preprocess(data, normalize, scale, log_transform, n_top_genes)\n",
    "    input_dim = processed_data.shape[1]\n",
    "\n",
    "    #define and train Model\n",
    "    print(\"Defining and Training Model\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    autoencoder = Autoencoder(input_dim, encoding_dim).to(device)\n",
    "    clustering_layer = ClusteringLayer(n_clusters, encoding_dim).to(device)\n",
    "    \n",
    "    #pretrain autoencoder\n",
    "    if ae_weights is None: #pretraining will be skipped if ae_weights is provided\n",
    "        print(\"Pretraining autoencoder...\")\n",
    "        pretrain_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=learning_rate) #adam optimizer\n",
    "        pretrain_criterion = torch.nn.MSELoss()\n",
    "        autoencoder.train() #set to train mode\n",
    "        for epoch in range(pretrain_epochs):\n",
    "            for batch in torch.utils.data.DataLoader(torch.tensor(processed_data, dtype=torch.float32).to(device), batch_size=batch_size, shuffle=True): #added explicit processed data to device\n",
    "                pretrain_optimizer.zero_grad()\n",
    "                _, decoded = autoencoder(batch)\n",
    "                loss = pretrain_criterion(decoded, batch)\n",
    "                loss.backward()\n",
    "                pretrain_optimizer.step()\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f\"Pretrain Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "        #torch.save(autoencoder.state_dict(), \"ae_weights.pt\") #saving weights for testing purposes\n",
    "    else:\n",
    "        autoencoder.load_state_dict(torch.load(ae_weights,map_location=torch.device('cpu'))) #added map location for device agnostic saving/loading\n",
    "    \n",
    "    #train jointly\n",
    "    trained_model, trained_clustering_layer = train(autoencoder, clustering_layer, processed_data, epochs, batch_size, learning_rate, n_clusters, device)\n",
    "\n",
    "    #get embeddings using the trained autoencoder\n",
    "    print(\"Obtaining Embeddings\")\n",
    "    embeddings = get_embeddings(trained_model, processed_data)\n",
    "    \n",
    "    #clustering using the trained clustering layer\n",
    "    print(\"Clustering\")\n",
    "    cluster_labels = trained_clustering_layer(torch.tensor(embeddings, dtype=torch.float32).to(device)).argmax(1).cpu().numpy()\n",
    "    \n",
    "    #eval\n",
    "    if labels is not None:  #real labels if available\n",
    "        true_labels = labels\n",
    "    else:\n",
    "        true_labels = np.random.randint(0, n_clusters, embeddings.shape[0]) #otherwise use placeholder\n",
    "\n",
    "    evaluation_results = evaluate_cluster(true_labels, cluster_labels)\n",
    "    print(f\"Clustering Evaluation: ARI={evaluation_results['ARI']:.4f}, NMI={evaluation_results['NMI']:.4f}\")\n",
    "    return cluster_labels, embeddings, scaler\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = 'scDeepClustering_Sample_Data/mouse_bladder_cell_select_2100.h5'\n",
    "    # filepath = 'Splatter_Sim_Data/splatter_simulate_data_1.h5'\n",
    "    file_format = 'h5'\n",
    "    n_clusters = 3\n",
    "    encoding_dim = 32\n",
    "    epochs = 100\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 32\n",
    "    normalize = True\n",
    "    scale = True\n",
    "    log_transform = True\n",
    "    n_top_genes = 2000\n",
    "    pretrain_epochs = 100 #can adjust\n",
    "    ae_weights = None #or \"ae_weights.pt\" #uncomment to use saved weights rather than pretraining every time\n",
    "    main(filepath, file_format, n_clusters, encoding_dim, epochs, learning_rate, batch_size, normalize, scale, log_transform, n_top_genes,pretrain_epochs,ae_weights) #pass to main function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
